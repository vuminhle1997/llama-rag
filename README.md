# LlamaRAG - Local LLM with RAG capabilities

A powerful application combining local Large Language Models with Retrieval-Augmented Generation (RAG), featuring both chat functionality and Azure API integration.

## 🚀 Features

- Local LLM support with customizable model selection
- RAG implementation for enhanced response accuracy
- Interactive chat interface
- Azure API integration for additional capabilities
- Modern, responsive UI built with NextJS and Tailwind

## 🛠️ Tech Stack

### Frontend
- NextJS
- Tailwind CSS
- ShadcnUI Components
- TypeScript

### Backend
- FastAPI
- ChromaDB for vector storage
- PostgreSQL for data persistence
- LlamaIndex for RAG implementation
- Local LLM integration (compatible with various models)
- Redis for user session management and caching

### Storage & Caching
- Local file system for document and image storage
- Efficient document processing pipeline

## 🏃‍♂️ Getting Started

1. Clone the repository
2. Install dependencies
3. Configure your environment variables
4. Start the development servers

## 📝 Prerequisites

- Node.js 20+
- Python 3.8+
- PostgreSQL
- Local LLM of choice
- Docker

## 🔧 Installation & Setup

//TODO:
Detailed setup instructions coming soon...

## 📄 License

MIT License