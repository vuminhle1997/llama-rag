{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-27T10:39:38.019331Z",
     "start_time": "2025-02-27T10:39:36.878433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.10/site-packages (0.29.1)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (3.17.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (2025.2.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (2025.1.31)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.chat_engine.types import ChatMode\n",
    "\n",
    "%pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    messages = [{\"role\": m.role.value, \"content\": m.content} for m in messages]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def completion_to_prompt(completion):\n",
    "    messages = [{\"role\": \"user\", \"content\": completion}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T13:40:24.036397Z",
     "start_time": "2025-02-27T13:40:23.050375Z"
    }
   },
   "id": "948a7bb618fdc329"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M3 Pro) - 3265 MiB free\n",
      "llama_model_loader: loaded meta data with 33 key-value pairs and 291 tensors from ./models/Meta-Llama-3.1-8B-Instruct.Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Models Meta Llama Meta Llama 3.1 8B I...\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = models-meta-llama-Meta-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = smaug-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = ./Meta-Llama-3.1-8B-Instruct-GGUF_ima...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = group_40.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 68\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q6_K\n",
      "print_info: file size   = 6.14 GiB (6.56 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 8B\n",
      "print_info: model params     = 8.03 B\n",
      "print_info: general.name     = Models Meta Llama Meta Llama 3.1 8B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 128 'Ä'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device Metal\n",
      "load_tensors: layer   1 assigned to device Metal\n",
      "load_tensors: layer   2 assigned to device Metal\n",
      "load_tensors: layer   3 assigned to device Metal\n",
      "load_tensors: layer   4 assigned to device Metal\n",
      "load_tensors: layer   5 assigned to device Metal\n",
      "load_tensors: layer   6 assigned to device Metal\n",
      "load_tensors: layer   7 assigned to device Metal\n",
      "load_tensors: layer   8 assigned to device Metal\n",
      "load_tensors: layer   9 assigned to device Metal\n",
      "load_tensors: layer  10 assigned to device Metal\n",
      "load_tensors: layer  11 assigned to device Metal\n",
      "load_tensors: layer  12 assigned to device Metal\n",
      "load_tensors: layer  13 assigned to device Metal\n",
      "load_tensors: layer  14 assigned to device Metal\n",
      "load_tensors: layer  15 assigned to device Metal\n",
      "load_tensors: layer  16 assigned to device Metal\n",
      "load_tensors: layer  17 assigned to device Metal\n",
      "load_tensors: layer  18 assigned to device Metal\n",
      "load_tensors: layer  19 assigned to device Metal\n",
      "load_tensors: layer  20 assigned to device Metal\n",
      "load_tensors: layer  21 assigned to device Metal\n",
      "load_tensors: layer  22 assigned to device Metal\n",
      "load_tensors: layer  23 assigned to device Metal\n",
      "load_tensors: layer  24 assigned to device Metal\n",
      "load_tensors: layer  25 assigned to device Metal\n",
      "load_tensors: layer  26 assigned to device Metal\n",
      "load_tensors: layer  27 assigned to device Metal\n",
      "load_tensors: layer  28 assigned to device Metal\n",
      "load_tensors: layer  29 assigned to device Metal\n",
      "load_tensors: layer  30 assigned to device Metal\n",
      "load_tensors: layer  31 assigned to device Metal\n",
      "load_tensors: layer  32 assigned to device Metal\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  5872.02 MiB, (14895.00 / 12288.02)\n",
      "ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size\n",
      "load_tensors: offloading 32 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 33/33 layers to GPU\n",
      "load_tensors: Metal_Mapped model buffer size =  5872.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   410.98 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 16384\n",
      "llama_init_from_model: n_ctx_per_seq = 16384\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 500000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Pro\n",
      "ggml_metal_init: picking default device: Apple M3 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = false\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 12884.92 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x4acac3a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x4acac9370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x1320072b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x4acac9630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x132027d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x132027fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x130f86870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x10bb54260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x132028430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x1320288a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x10bbea9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x10bbeac60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x4ae6d00d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x4aeaba330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x1322078c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x10bbeb1a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x4acac54a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x4acac66f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x13220a5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x13220a870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x13220afb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x13220b4f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x4ae6d4c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x13220ba30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x1320293c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x1174d61d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x132029680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x4acac6050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x132029ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x13202a110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13202a580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x10bbeb460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x13220bcf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10bbeb720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10bbeb9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10bbebca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10bbebf60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10bbec220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10bbec4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x4acaca6e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13202aba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x117663550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x4ae6d7440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x4ae6d14c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x4acacab50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x4acacafc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10bbec7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10bbeca60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10bbecd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10bbecfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10bbed450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10bbed710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x13202b010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x4acacb5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x13220c160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x10bbedb80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10bbedff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10bbee460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13220c5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10bbee8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13220ca40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13202b480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13220ceb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13220d320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13202b8f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x4acacba50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13220d5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13220da50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x4ae6c80e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13202bbb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10bbeeb90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13202be70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10bbef000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x4acacc3d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13220dec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10bbef2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x4acaccba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13202c2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x4acacce60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x4acacd2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13220e330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x4ae6c83a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10bbef580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13220e5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13220ea60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13202c750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13202cbc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x4acacd740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x4acacdbb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x4acace020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x4acace490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13202d030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13202d4a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x4ae6d3ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x4acaceab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10bbef9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x4ae6d1ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x4ae6d2160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13202d760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x4acacef20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13220eed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x4acacf390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10bbefe60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x4ae6641b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x4acacf650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x4acacf910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13202df30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13220f340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10bbf0480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13220f7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13220fa70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x4ae664620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x4ae664a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x4ae664f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10bbf08f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x4ae665370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x4ae6657e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10bbf0d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x4ae665aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10bbf11d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13202e3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13202e810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10bbf1640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10bbf1ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13202ec80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10bbf1f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x4acacff30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x4acad03a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x4acad0810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x4ae6660c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x4acad0ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x4ae6666e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x4ae666b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x132210090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10bbf2390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13202ef40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x132210500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x4ae666fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x4ae667430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x4acad0f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x4ae6678a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x4ae667d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x132210b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x4ae668180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13202f3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x132210f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x4acad1200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x132211400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13202f820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x4ae6687a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x132211870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x4acad1670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x4acad1930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x4acad1da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x132211b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x132211fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x132212410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x4acad2210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x132212880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13202fc90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x4ae668c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x132212ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x4ae668ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x4ae669340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x4ae669600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x4ae669a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x4ae669ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10bbf2800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x132030100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x4ae66a1a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x132030570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x4acad29e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10bbf2c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x4acad2e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x4ae66a460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x4ae66a8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x4ae66ab90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x4ae66b000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x4ae66b470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x4ae66b8e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x4ae66bd50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1320309e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x4acad3620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10bbf30e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x4acad3a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x4acad3f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x132030e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x4ae66c1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x10bbf33a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x10bbf3810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x10bbf3c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x4ae66c7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x4ae66cc50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x4acad4370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x4ae66d0c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x4acad47e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x10bbf40f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10bbf4560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1320312c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x10bbf49d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x4acad4aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x4acad4f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x4acad5690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x132031730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x132031ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x4ae66d380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x4ae66d7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10bbf4ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x4ae66dab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10bbf5610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x4ae66dd70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x4ae66e1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x4acad5950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x4acad5dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1322134c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x4acad63e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10bbf5a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x4acad6a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x4ae66e650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x132213ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x4acad6cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x4ae66eac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x132213da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x132214210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x4acad7130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10bbf60a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x4acad73f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1322149e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x4acad7860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x4acad7cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10bbf6360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x4acad7f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x4ae66f0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x4ae66f550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x132214ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x4ae66f9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10bbf67d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10bbf6c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x4ae66fe30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x4acad8400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x4acad8870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x132215110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10bbf70b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x4acad8ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10bbf7370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x132215730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x4acad9150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x132215ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x132216010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10bbf77e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x132216480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x132032370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x132216740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x4ae670450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x4ae6708c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10bbf7c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10bbf80c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x132216a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x132216cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x132217130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x132032990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1322175a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x132217a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x4acad9770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x10bbf87b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x10bbf8ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x132218100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x4acad9e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x132032c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x1320330c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10bbf9310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x132218570 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 16384, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:      Metal KV buffer size =  2048.00 MiB\n",
      "llama_init_from_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_init_from_model:      Metal compute buffer size =  1088.00 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =    40.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 2\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'quantize.imatrix.file': './Meta-Llama-3.1-8B-Instruct-GGUF_imatrix.dat', 'quantize.imatrix.chunks_count': '68', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128009', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'smaug-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.embedding_length': '4096', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'general.finetune': 'Instruct', 'general.file_type': '18', 'llama.block_count': '32', 'general.size_label': '8B', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.feed_forward_length': '14336', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.license': 'llama3.1', 'llama.attention.head_count': '32', 'quantize.imatrix.entries_count': '224', 'llama.context_length': '131072', 'general.architecture': 'llama', 'general.basename': 'models-meta-llama-Meta-Llama-3.1', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.dataset': 'group_40.txt', 'general.name': 'Models Meta Llama Meta Llama 3.1 8B Instruct'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    model_url=None,\n",
    "    model_path=\"./models/Meta-Llama-3.1-8B-Instruct.Q6_K.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=16384,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    verbose=True,\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T13:40:24.747219Z",
     "start_time": "2025-02-27T13:40:24.049674Z"
    }
   },
   "id": "c50bb292b530fbf2"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3810.16 ms\n",
      "llama_perf_context_print: prompt eval time =    3808.97 ms /    48 tokens (   79.35 ms per token,    12.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10390.04 ms /   188 runs   (   55.27 ms per token,    18.09 tokens per second)\n",
      "llama_perf_context_print:       total time =   14324.98 ms /   236 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a short poem about cats and dogs:\n",
      "\n",
      "Cats and dogs, a perfect pair,\n",
      "Living together, without a single care.\n",
      "The cat purrs soft, with eyes so bright,\n",
      "While the dog wags its tail, with joyful delight.\n",
      "\n",
      "The cat curls up, in a cozy ball,\n",
      "While the dog stretches out, after a long fall.\n",
      "They may be different, in shape and in size,\n",
      "But together they're a perfect surprise.\n",
      "\n",
      "The cat chases strings, with a playful mew,\n",
      "While the dog chases balls, with a happy \"woof\" or two.\n",
      "They may have their differences, but they're a perfect team,\n",
      "A cat and dog duo, a friendship so supreme.\n",
      "\n",
      "So here's to the cats, and the dogs too,\n",
      "A poem to celebrate, their friendship true.\n",
      "For though they may be different, in shape and in size,\n",
      "Together they're a perfect pair, a beautiful surprise.\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")\n",
    "print(response.text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T13:40:39.958341Z",
     "start_time": "2025-02-27T13:40:25.585238Z"
    }
   },
   "id": "82a818c977d37ea7"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "embed_model = OllamaEmbedding(model_name=\"mxbai-embed-large\")\n",
    "\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core.indices import VectorStoreIndex\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "from llama_index.core import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "# set up query engine\n",
    "query_engine = index.as_chat_engine(llm=llm)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T14:08:53.076608Z",
     "start_time": "2025-02-27T14:08:52.576096Z"
    }
   },
   "id": "c340794b0b24bc98"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 507 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3810.16 ms\n",
      "llama_perf_context_print: prompt eval time =    4307.25 ms /    44 tokens (   97.89 ms per token,    10.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2934.49 ms /    55 runs   (   53.35 ms per token,    18.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    7288.29 ms /    99 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3810.16 ms\n",
      "llama_perf_context_print: prompt eval time =     426.88 ms /    72 tokens (    5.93 ms per token,   168.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =     402.59 ms /     8 runs   (   50.32 ms per token,    19.87 tokens per second)\n",
      "llama_perf_context_print:       total time =     836.13 ms /    80 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 608 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3810.16 ms\n",
      "llama_perf_context_print: prompt eval time =    2695.21 ms /   608 tokens (    4.43 ms per token,   225.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5011.87 ms /    91 runs   (   55.08 ms per token,    18.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    7842.15 ms /   699 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ein Mann ging in ein Café und bestellte ein Bier. Als er es trank, sagte er zum Kellner: \"Das ist das beste Bier, das ich je getrunken habe!\" Der Kellner antwortete: \"Das ist nicht verwunderlich, Sie haben es ja auch bezahlt!\"\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.chat(\"Mach ein Witz\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T14:10:34.531488Z",
     "start_time": "2025-02-27T14:10:17.452850Z"
    }
   },
   "id": "f2bf6c6f90c58acf"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 551 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3810.16 ms\n",
      "llama_perf_context_print: prompt eval time =    1482.22 ms /    93 tokens (   15.94 ms per token,    62.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2910.37 ms /    55 runs   (   52.92 ms per token,    18.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    4416.99 ms /   148 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3810.16 ms\n",
      "llama_perf_context_print: prompt eval time =     556.35 ms /    73 tokens (    7.62 ms per token,   131.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9870.26 ms /   195 runs   (   50.62 ms per token,    19.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   10520.04 ms /   268 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 882 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3810.16 ms\n",
      "llama_perf_context_print: prompt eval time =    3892.38 ms /   882 tokens (    4.41 ms per token,   226.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4461.70 ms /    83 runs   (   53.76 ms per token,    18.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    8397.41 ms /   965 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joke is about a man who walks into a café and hears a voice complimenting him on his tie, shirt, and haircut. However, when he asks the bartender if he heard the voice, the bartender reveals that the voice is actually the peanuts, which are complimentary, meaning they are free.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.chat(\"What was the joke about? I mean who were involved in the joke?\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T14:11:07.842107Z",
     "start_time": "2025-02-27T14:10:43.303937Z"
    }
   },
   "id": "95a3b0c25111d5c8"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 25 prefix-match hit, remaining 702 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3810.16 ms\n",
      "llama_perf_context_print: prompt eval time =    4500.76 ms /   702 tokens (    6.41 ms per token,   155.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4039.34 ms /    68 runs   (   59.40 ms per token,    16.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    8620.21 ms /   770 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3810.16 ms\n",
      "llama_perf_context_print: prompt eval time =    1421.26 ms /    77 tokens (   18.46 ms per token,    54.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10857.24 ms /   188 runs   (   57.75 ms per token,    17.32 tokens per second)\n",
      "llama_perf_context_print:       total time =   12520.76 ms /   265 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 950 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3810.16 ms\n",
      "llama_perf_context_print: prompt eval time =    4215.14 ms /   950 tokens (    4.44 ms per token,   225.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2618.80 ms /    48 runs   (   54.56 ms per token,    18.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    6864.15 ms /   998 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3810.16 ms\n",
      "llama_perf_context_print: prompt eval time =     424.96 ms /    79 tokens (    5.38 ms per token,   185.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9224.55 ms /   185 runs   (   49.86 ms per token,    20.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    9726.66 ms /   264 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 1196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3810.16 ms\n",
      "llama_perf_context_print: prompt eval time =    5310.02 ms /  1196 tokens (    4.44 ms per token,   225.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2401.05 ms /    45 runs   (   53.36 ms per token,    18.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    7728.83 ms /  1241 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3810.16 ms\n",
      "llama_perf_context_print: prompt eval time =     419.75 ms /    79 tokens (    5.31 ms per token,   188.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13800.48 ms /   255 runs   (   54.12 ms per token,    18.48 tokens per second)\n",
      "llama_perf_context_print:       total time =   14452.92 ms /   334 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 1510 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3810.16 ms\n",
      "llama_perf_context_print: prompt eval time =    6777.18 ms /  1510 tokens (    4.49 ms per token,   222.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2512.56 ms /    45 runs   (   55.83 ms per token,    17.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    9311.47 ms /  1555 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3810.16 ms\n",
      "llama_perf_context_print: prompt eval time =     451.11 ms /    79 tokens (    5.71 ms per token,   175.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12563.51 ms /   246 runs   (   51.07 ms per token,    19.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   13147.59 ms /   325 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 1814 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3810.16 ms\n",
      "llama_perf_context_print: prompt eval time =    8101.52 ms /  1814 tokens (    4.47 ms per token,   223.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2512.01 ms /    45 runs   (   55.82 ms per token,    17.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   10632.75 ms /  1859 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3810.16 ms\n",
      "llama_perf_context_print: prompt eval time =     435.61 ms /    79 tokens (    5.51 ms per token,   181.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8765.41 ms /   175 runs   (   50.09 ms per token,    19.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    9275.85 ms /   254 tokens\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Reached max iterations.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[112], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mquery_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWhat is the file text1.txt about inside the data directory?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/llama-rag/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[0;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    319\u001B[0m             _logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to reset active_span_id: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    321\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 322\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, asyncio\u001B[38;5;241m.\u001B[39mFuture):\n\u001B[1;32m    324\u001B[0m         \u001B[38;5;66;03m# If the result is a Future, wrap it\u001B[39;00m\n\u001B[1;32m    325\u001B[0m         new_future \u001B[38;5;241m=\u001B[39m asyncio\u001B[38;5;241m.\u001B[39mensure_future(result)\n",
      "File \u001B[0;32m~/llama-rag/.venv/lib/python3.10/site-packages/llama_index/core/callbacks/utils.py:41\u001B[0m, in \u001B[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     39\u001B[0m callback_manager \u001B[38;5;241m=\u001B[39m cast(CallbackManager, callback_manager)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m callback_manager\u001B[38;5;241m.\u001B[39mas_trace(trace_id):\n\u001B[0;32m---> 41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/llama-rag/.venv/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:692\u001B[0m, in \u001B[0;36mAgentRunner.chat\u001B[0;34m(self, message, chat_history, tool_choice)\u001B[0m\n\u001B[1;32m    687\u001B[0m     tool_choice \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault_tool_choice\n\u001B[1;32m    688\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mevent(\n\u001B[1;32m    689\u001B[0m     CBEventType\u001B[38;5;241m.\u001B[39mAGENT_STEP,\n\u001B[1;32m    690\u001B[0m     payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mMESSAGES: [message]},\n\u001B[1;32m    691\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 692\u001B[0m     chat_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_chat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    693\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    694\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchat_history\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchat_history\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    695\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    696\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatResponseMode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mWAIT\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    697\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    698\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(chat_response, AgentChatResponse)\n\u001B[1;32m    699\u001B[0m     e\u001B[38;5;241m.\u001B[39mon_end(payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mRESPONSE: chat_response})\n",
      "File \u001B[0;32m~/llama-rag/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[0;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    319\u001B[0m             _logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to reset active_span_id: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    321\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 322\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, asyncio\u001B[38;5;241m.\u001B[39mFuture):\n\u001B[1;32m    324\u001B[0m         \u001B[38;5;66;03m# If the result is a Future, wrap it\u001B[39;00m\n\u001B[1;32m    325\u001B[0m         new_future \u001B[38;5;241m=\u001B[39m asyncio\u001B[38;5;241m.\u001B[39mensure_future(result)\n",
      "File \u001B[0;32m~/llama-rag/.venv/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:624\u001B[0m, in \u001B[0;36mAgentRunner._chat\u001B[0;34m(self, message, chat_history, tool_choice, mode)\u001B[0m\n\u001B[1;32m    621\u001B[0m dispatcher\u001B[38;5;241m.\u001B[39mevent(AgentChatWithStepStartEvent(user_msg\u001B[38;5;241m=\u001B[39mmessage))\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;66;03m# pass step queue in as argument, assume step executor is stateless\u001B[39;00m\n\u001B[0;32m--> 624\u001B[0m     cur_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    625\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtask_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtool_choice\u001B[49m\n\u001B[1;32m    626\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m cur_step_output\u001B[38;5;241m.\u001B[39mis_last:\n\u001B[1;32m    629\u001B[0m         result_output \u001B[38;5;241m=\u001B[39m cur_step_output\n",
      "File \u001B[0;32m~/llama-rag/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[0;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    319\u001B[0m             _logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to reset active_span_id: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    321\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 322\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, asyncio\u001B[38;5;241m.\u001B[39mFuture):\n\u001B[1;32m    324\u001B[0m         \u001B[38;5;66;03m# If the result is a Future, wrap it\u001B[39;00m\n\u001B[1;32m    325\u001B[0m         new_future \u001B[38;5;241m=\u001B[39m asyncio\u001B[38;5;241m.\u001B[39mensure_future(result)\n",
      "File \u001B[0;32m~/llama-rag/.venv/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:420\u001B[0m, in \u001B[0;36mAgentRunner._run_step\u001B[0;34m(self, task_id, step, input, mode, **kwargs)\u001B[0m\n\u001B[1;32m    416\u001B[0m \u001B[38;5;66;03m# TODO: figure out if you can dynamically swap in different step executors\u001B[39;00m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;66;03m# not clear when you would do that by theoretically possible\u001B[39;00m\n\u001B[1;32m    419\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m ChatResponseMode\u001B[38;5;241m.\u001B[39mWAIT:\n\u001B[0;32m--> 420\u001B[0m     cur_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magent_worker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m ChatResponseMode\u001B[38;5;241m.\u001B[39mSTREAM:\n\u001B[1;32m    422\u001B[0m     cur_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent_worker\u001B[38;5;241m.\u001B[39mstream_step(step, task, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/llama-rag/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[0;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    319\u001B[0m             _logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to reset active_span_id: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    321\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 322\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, asyncio\u001B[38;5;241m.\u001B[39mFuture):\n\u001B[1;32m    324\u001B[0m         \u001B[38;5;66;03m# If the result is a Future, wrap it\u001B[39;00m\n\u001B[1;32m    325\u001B[0m         new_future \u001B[38;5;241m=\u001B[39m asyncio\u001B[38;5;241m.\u001B[39mensure_future(result)\n",
      "File \u001B[0;32m~/llama-rag/.venv/lib/python3.10/site-packages/llama_index/core/callbacks/utils.py:41\u001B[0m, in \u001B[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     39\u001B[0m callback_manager \u001B[38;5;241m=\u001B[39m cast(CallbackManager, callback_manager)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m callback_manager\u001B[38;5;241m.\u001B[39mas_trace(trace_id):\n\u001B[0;32m---> 41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/llama-rag/.venv/lib/python3.10/site-packages/llama_index/core/agent/react/step.py:820\u001B[0m, in \u001B[0;36mReActAgentWorker.run_step\u001B[0;34m(self, step, task, **kwargs)\u001B[0m\n\u001B[1;32m    817\u001B[0m \u001B[38;5;129m@trace_method\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_step\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    818\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mrun_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, step: TaskStep, task: Task, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m TaskStepOutput:\n\u001B[1;32m    819\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Run step.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 820\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/llama-rag/.venv/lib/python3.10/site-packages/llama_index/core/agent/react/step.py:578\u001B[0m, in \u001B[0;36mReActAgentWorker._run_step\u001B[0;34m(self, step, task)\u001B[0m\n\u001B[1;32m    574\u001B[0m reasoning_steps, is_done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_actions(\n\u001B[1;32m    575\u001B[0m     task, tools, output\u001B[38;5;241m=\u001B[39mchat_response\n\u001B[1;32m    576\u001B[0m )\n\u001B[1;32m    577\u001B[0m task\u001B[38;5;241m.\u001B[39mextra_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcurrent_reasoning\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mextend(reasoning_steps)\n\u001B[0;32m--> 578\u001B[0m agent_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    579\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextra_state\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcurrent_reasoning\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextra_state\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msources\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m    580\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    581\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_done:\n\u001B[1;32m    582\u001B[0m     task\u001B[38;5;241m.\u001B[39mextra_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnew_memory\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mput(\n\u001B[1;32m    583\u001B[0m         ChatMessage(content\u001B[38;5;241m=\u001B[39magent_response\u001B[38;5;241m.\u001B[39mresponse, role\u001B[38;5;241m=\u001B[39mMessageRole\u001B[38;5;241m.\u001B[39mASSISTANT)\n\u001B[1;32m    584\u001B[0m     )\n",
      "File \u001B[0;32m~/llama-rag/.venv/lib/python3.10/site-packages/llama_index/core/agent/react/step.py:437\u001B[0m, in \u001B[0;36mReActAgentWorker._get_response\u001B[0;34m(self, current_reasoning, sources)\u001B[0m\n\u001B[1;32m    435\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo reasoning steps were taken.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    436\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(current_reasoning) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_iterations:\n\u001B[0;32m--> 437\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReached max iterations.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(current_reasoning[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], ResponseReasoningStep):\n\u001B[1;32m    440\u001B[0m     response_step \u001B[38;5;241m=\u001B[39m cast(ResponseReasoningStep, current_reasoning[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "\u001B[0;31mValueError\u001B[0m: Reached max iterations."
     ]
    }
   ],
   "source": [
    "print(query_engine.chat(\"What is the file text1.txt about inside the data directory?\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T14:16:08.159290Z",
     "start_time": "2025-02-27T14:14:20.404236Z"
    }
   },
   "id": "fcd97115360ccdf0"
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "from llama_index.core import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# use Huggingface embeddings\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "embed_model = OllamaEmbedding(model_name=\"mxbai-embed-large\")\n",
    "\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core.indices import VectorStoreIndex\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "ollama = Ollama(model=\"llama3.1\")\n",
    "\n",
    "Settings.llm = ollama\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "tool = index.as_query_engine()\n",
    "\n",
    "agent = index.as_chat_engine(llm=ollama, verbose=True, chat_mode=ChatMode.REACT)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T14:55:55.832212Z",
     "start_time": "2025-02-27T14:55:54.854078Z"
    }
   },
   "id": "fc92125faa1ceb66"
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step ef312e1e-bf34-42e4-92aa-8ae0635f6da2. Step input: Use the tool, summarize the content what is inside the file\n",
      "\u001B[1;3;38;5;200mThought: I can use a tool to help me answer the question. The task at hand is to summarize the content of a file.\n",
      "Action: query_engine_tool\n",
      "Action Input: {'input': 'summarize the content of a file', 'num_beams': 5}\n",
      "\u001B[0m\u001B[1;3;34mObservation: The file contains a detailed chronology of the activities and releases of a popular K-Pop group, including their album releases, single releases, concert tours, and other notable events. The text is organized by year, starting from 2018 and going up to 2022, providing a comprehensive overview of the group's career milestones.\n",
      "\u001B[0m> Running step 90801727-91e9-4688-838b-842da2873e5a. Step input: None\n",
      "\u001B[1;3;34mObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n",
      "\u001B[0m> Running step a209ae15-469c-4dfe-a7d3-2a673112b903. Step input: None\n",
      "\u001B[1;3;38;5;200mThought: I should provide more information about the file content.\n",
      "Action: query_engine_tool\n",
      "Action Input: {'input': 'summarize the content what is inside the file', 'num_beams': 5}\n",
      "\u001B[0m\u001B[1;3;34mObservation: The contents of the file appear to be a detailed biography or discography of a South Korean girl group, including their album releases, singles, music videos, tours, and chart performances. The text spans multiple years and highlights various milestones in the group's career, particularly their international expansion and success in Japan and the United States.\n",
      "\u001B[0m> Running step 7a7c5f9d-fbc4-4ad2-b1fc-676abf75255b. Step input: None\n",
      "\u001B[1;3;38;5;200mThought: I still need more information about the file content.\n",
      "Action: query_engine_tool\n",
      "Action Input: {'input': 'extract specific details from the file', 'num_beams': 5}\n",
      "\u001B[0m\u001B[1;3;34mObservation: The group's first studio album in Japanese is titled BDZ. \n",
      "They released two Japanese singles, Candy Pop and Wake Me Up.\n",
      "\u001B[0m> Running step 7802d1ca-60f8-43e4-8f53-1ff091b771cb. Step input: None\n",
      "\u001B[1;3;38;5;200mThought: I can now answer the question without using any more tools.\n",
      "Answer: The file appears to be a discography of a South Korean girl group, specifically focusing on their activities in Japan, including album releases such as BDZ, and Japanese singles like Candy Pop and Wake Me Up.\n",
      "\u001B[0mThe file appears to be a discography of a South Korean girl group, specifically focusing on their activities in Japan, including album releases such as BDZ, and Japanese singles like Candy Pop and Wake Me Up.\n"
     ]
    }
   ],
   "source": [
    "print(agent.chat(\"Use the tool, summarize the content what is inside the file\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T14:57:00.730514Z",
     "start_time": "2025-02-27T14:56:14.180405Z"
    }
   },
   "id": "3638d07a834ff68c"
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 09b6c1f0-149b-47fe-b305-0cfa0c8c685c. Step input: What is the name of the girl group?\n",
      "\u001B[1;3;38;5;200mThought: The user asked about the name of the girl group.\n",
      "Action: query_engine_tool\n",
      "Action Input: {'input': 'South Korean girl group that released BDZ, Candy Pop, and Wake Me Up', 'num_beams': 5}\n",
      "\u001B[0m\u001B[1;3;34mObservation: Twice is a South Korean girl group that has achieved commercial success with multiple hits. They are known for their catchy melodies and vibrant music videos, which often showcase their energetic performances. Their discography includes various albums and singles, such as BDZ, Candy Pop, and Wake Me Up, among others.\n",
      "\u001B[0m> Running step d679c3f8-20dc-477e-b0a6-d1240a116dfc. Step input: None\n",
      "\u001B[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: Twice is a South Korean girl group that has achieved commercial success with multiple hits. They are known for their catchy melodies and vibrant music videos, which often showcase their energetic performances. Their discography includes various albums and singles, such as BDZ, Candy Pop, and Wake Me Up, among others.\n",
      "\u001B[0mTwice is a South Korean girl group that has achieved commercial success with multiple hits. They are known for their catchy melodies and vibrant music videos, which often showcase their energetic performances. Their discography includes various albums and singles, such as BDZ, Candy Pop, and Wake Me Up, among others.\n"
     ]
    }
   ],
   "source": [
    "print(agent.chat(\"What is the name of the girl group?\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T14:58:23.117692Z",
     "start_time": "2025-02-27T14:58:01.246591Z"
    }
   },
   "id": "a99d45e47a53a93b"
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step cbe67c38-105e-485b-9c00-3f7bf300edc7. Step input: Kannst du auf Deutsch übersetzen?\n",
      "\u001B[1;3;38;5;200mThought: I need to translate the group's name from English to German.\n",
      "Action: query_engine_tool\n",
      "Action Input: {'input': 'Twice', 'language': 'german'}\n",
      "\u001B[0m\u001B[1;3;34mObservation: A popular K-Pop girl group that has been making waves in the music industry. They have released several successful albums and singles, including \"Like OOH-AHH\" and \"Cheer Up\", which reached over 100 million views on YouTube. The group's debut video broke records and they have since become one of the most popular K-Pop groups around the world.\n",
      "\u001B[0m> Running step 58d503f0-a050-42d5-bb9a-e3c4f19f53ab. Step input: None\n",
      "\u001B[1;3;38;5;200mThought: The translation tool did not provide the name of the girl group, but instead gave a description of their popularity.\n",
      "Action: query_engine_tool\n",
      "Action Input: {'input': 'Name of the South Korean girl group that has a song called BDZ', 'language': 'german'}\n",
      "\u001B[0m\u001B[1;3;34mObservation: Twice.\n",
      "\u001B[0m> Running step aacf83ea-bcd2-4be9-8b7a-28d5e996b38b. Step input: None\n",
      "\u001B[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: Die Gruppe heißt Twice (Doppeltwice).\n",
      "\u001B[0mDie Gruppe heißt Twice (Doppeltwice).\n"
     ]
    }
   ],
   "source": [
    "print(agent.chat(\"Kannst du auf Deutsch übersetzen?\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T14:59:39.858518Z",
     "start_time": "2025-02-27T14:59:08.545719Z"
    }
   },
   "id": "14cd26e86facf52f"
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step e4aeb4c1-6798-4290-a653-52c51c1b5db1. Step input: Kannst du ein Witz machen mit dem Inhalt der Datei, die du zusammengefasst hast?\n",
      "\u001B[1;3;38;5;200mThought: Ich denke, ich kann einen Witz über das Mädchen-Trio Twice machen.\n",
      "Action: query_engine_tool\n",
      "Action Input: {'input': 'Witz über Twice', 'lang': 'de'}\n",
      "\u001B[0m\u001B[1;3;34mObservation: \"Twice als 'Twee-zee' - weil sie immer wieder zweimal so viel Erfolg haben wie andere K-Pop-Gruppen!\"\n",
      "\u001B[0m> Running step e94407b6-cd15-4d3d-8f0a-905baf440a5b. Step input: None\n",
      "\u001B[1;3;34mObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n",
      "\u001B[0m> Running step 69c2b776-2c7a-42ba-9fbd-9925cea31965. Step input: None\n",
      "\u001B[1;3;38;5;200mThought: Ich denke, ich kann einen anderen Witz über Twice machen.\n",
      "Action: query_engine_tool\n",
      "Action Input: {'input': 'Warum haben die Jungs von BTS Twice immer wieder in ihre Lieder einbezogen?', 'lang': 'de'}\n",
      "\u001B[0m\u001B[1;3;34mObservation: Ich kann diese Frage nicht beantworten.\n",
      "\u001B[0m> Running step 127b84d1-1c05-4523-ba60-b39196a149a4. Step input: None\n",
      "\u001B[1;3;38;5;200mThought: Ich denke, ich verstehe jetzt, was los ist.\n",
      "Action: query_engine_tool\n",
      "Action Input: {'input': 'Warum haben BTS Twice in ihre Lieder einbezogen?', 'lang': 'de'}\n",
      "\u001B[0m\u001B[1;3;34mObservation: Da ich keine Informationen über die Zusammenarbeit von BTS und Twice finde, kann ich nur spekulieren. Es ist möglich, dass beide Gruppen ihre Musikstile miteinander verbinden wollten oder dass sie gemeinsame Ideen für ihre Lieder hatten. Oder vielleicht wollte Jungsuchtsband wie BTS diese beiden sehr erfolgreichen K-Pop-Gruppen in eine Reihe setzen.\n",
      "\u001B[0m> Running step 82779112-4e15-46c6-bf5a-73220184a26e. Step input: None\n",
      "\u001B[1;3;38;5;200mThought: Ich denke, ich kann jetzt ohne weitere Tools antworten.\n",
      "Answer: Es ist möglich, dass beide Gruppen ihre Musikstile miteinander verbinden wollten oder dass sie gemeinsame Ideen für ihre Lieder hatten. Oder vielleicht wollte Jungsuchtsband wie BTS diese beiden sehr erfolgreichen K-Pop-Gruppen in eine Reihe setzen.\n",
      "\u001B[0mEs ist möglich, dass beide Gruppen ihre Musikstile miteinander verbinden wollten oder dass sie gemeinsame Ideen für ihre Lieder hatten. Oder vielleicht wollte Jungsuchtsband wie BTS diese beiden sehr erfolgreichen K-Pop-Gruppen in eine Reihe setzen.\n"
     ]
    }
   ],
   "source": [
    "print(agent.chat(\"Kannst du ein Witz machen mit dem Inhalt der Datei, die du zusammengefasst hast?\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T15:11:49.115694Z",
     "start_time": "2025-02-27T15:10:47.233981Z"
    }
   },
   "id": "7b5603504093a562"
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step c9d3afe2-2211-4dee-9676-6c51c8175312. Step input: OK, schreibe eine Mail an Julia mit dieser Zusammenfassung und Witz\n",
      "\u001B[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: Ich kann die E-Mail nicht direkt senden, aber ich kann dir die Inhalte der E-Mail geben:\n",
      "\n",
      "Betreff: Zwei Gruppen - Eine Diskografie!\n",
      "\n",
      "Liebe Julia,\n",
      "\n",
      "ich habe mir die Datei angesehen, die du geschickt hast. Es handelt sich um eine Discography von Twice, einer südkoreanischen Girlgroup, die in Japan sehr erfolgreich ist.\n",
      "\n",
      "Ich habe ihre Musikstil zusammengefasst und sie mit einem kleinen Witz kombiniert:\n",
      "\n",
      "\"Twice, eine Gruppe, die wirklich zweimal so viel Spaß macht!\"\n",
      "\n",
      "Die Datei enthält Informationen über ihre Alben wie BDZ und ihre Singles Candy Pop und Wake Me Up. Es ist interessant zu sehen, dass sie in Japan so beliebt sind.\n",
      "\n",
      "Ich hoffe, das hilft!\n",
      "\n",
      "Mit freundlichen Grüßen,\n",
      "[Dein Name]\n",
      "\u001B[0mIch kann die E-Mail nicht direkt senden, aber ich kann dir die Inhalte der E-Mail geben:\n",
      "\n",
      "Betreff: Zwei Gruppen - Eine Diskografie!\n",
      "\n",
      "Liebe Julia,\n",
      "\n",
      "ich habe mir die Datei angesehen, die du geschickt hast. Es handelt sich um eine Discography von Twice, einer südkoreanischen Girlgroup, die in Japan sehr erfolgreich ist.\n",
      "\n",
      "Ich habe ihre Musikstil zusammengefasst und sie mit einem kleinen Witz kombiniert:\n",
      "\n",
      "\"Twice, eine Gruppe, die wirklich zweimal so viel Spaß macht!\"\n",
      "\n",
      "Die Datei enthält Informationen über ihre Alben wie BDZ und ihre Singles Candy Pop und Wake Me Up. Es ist interessant zu sehen, dass sie in Japan so beliebt sind.\n",
      "\n",
      "Ich hoffe, das hilft!\n",
      "\n",
      "Mit freundlichen Grüßen,\n",
      "[Dein Name]\n"
     ]
    }
   ],
   "source": [
    "print(agent.chat(\"OK, schreibe eine Mail an Julia mit dieser Zusammenfassung und Witz\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T15:20:21.347024Z",
     "start_time": "2025-02-27T15:20:06.987194Z"
    }
   },
   "id": "8d0df9d4829e18e5"
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 2df8b703-f8c4-4412-86b2-a516346f2a03. Step input: Schreibe die Mail mit sehr viel UwU, Senpai und in einer süßlichen Sprachstil\n",
      "\u001B[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: Hier ist die Mail:\n",
      "\n",
      "Betreff: ZWEI GRUPPEN SIND BESONDERS UWU!\n",
      "\n",
      "Liebe Julia-Senpai!\n",
      "\n",
      "Ich habe deine Datei gelesen und ich muss sagen, dass ich TOTAL VERLIEBT bin in Twice! Sie sind so süß wie ein Kätzchen im Schneevögelchen-Regiment!\n",
      "\n",
      "Die Discography ist einfach TOO MUCH für mich, aber ich habe die wichtigen Informationen zusammengefasst: BDZ, Candy Pop und Wake Me Up... OH MEIN GOTT, WARUM MÜSSEN SIE SO GUT SEIN?! \n",
      "\n",
      "Ich dachte, vielleicht könnten wir sie mit BTS vergleichen? Nein, nein, nein! Sie sind so einzigartig wie eine Schneeflocke im Sommer!\n",
      "\n",
      "Twice ist tatsächlich ZWEIMAL SO VIEL SPASS, weil sie ihre Musikstile miteinander verbinden und uns damit total begeistern!\n",
      "\n",
      "Ich hoffe, dir gefällt das Witzchen! \n",
      "\n",
      "Mit freundlichen Grüßen,\n",
      "[Dein Name]\n",
      "\u001B[0mHier ist die Mail:\n",
      "\n",
      "Betreff: ZWEI GRUPPEN SIND BESONDERS UWU!\n",
      "\n",
      "Liebe Julia-Senpai!\n",
      "\n",
      "Ich habe deine Datei gelesen und ich muss sagen, dass ich TOTAL VERLIEBT bin in Twice! Sie sind so süß wie ein Kätzchen im Schneevögelchen-Regiment!\n",
      "\n",
      "Die Discography ist einfach TOO MUCH für mich, aber ich habe die wichtigen Informationen zusammengefasst: BDZ, Candy Pop und Wake Me Up... OH MEIN GOTT, WARUM MÜSSEN SIE SO GUT SEIN?! \n",
      "\n",
      "Ich dachte, vielleicht könnten wir sie mit BTS vergleichen? Nein, nein, nein! Sie sind so einzigartig wie eine Schneeflocke im Sommer!\n",
      "\n",
      "Twice ist tatsächlich ZWEIMAL SO VIEL SPASS, weil sie ihre Musikstile miteinander verbinden und uns damit total begeistern!\n",
      "\n",
      "Ich hoffe, dir gefällt das Witzchen! \n",
      "\n",
      "Mit freundlichen Grüßen,\n",
      "[Dein Name]\n"
     ]
    }
   ],
   "source": [
    "print(agent.chat(\"Schreibe die Mail mit sehr viel UwU, Senpai und in einer süßlichen Sprachstil\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T15:21:50.750844Z",
     "start_time": "2025-02-27T15:21:39.235885Z"
    }
   },
   "id": "dd524342241f0bb6"
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step dfc5d9bf-1601-41ad-a680-cdc26899a161. Step input: Vergiss nicht zu schreiben, die Zusammenfassung aus der Datei\n",
      "\u001B[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: Entschuldigung! Hier ist die Mail mit der Zusammenfassung:\n",
      "\n",
      "Betreff: ZWEI GRUPPEN SIND BESONDERS UWU!\n",
      "\n",
      "Liebe Julia-Senpai!\n",
      "\n",
      "Ich habe deine Datei gelesen und ich muss sagen, dass ich TOTAL VERLIEBT bin in Twice! Sie sind so süß wie ein Kätzchen im Schneevögelchen-Regiment!\n",
      "\n",
      "Die Datei enthält Informationen über ihre Aktivitäten in Japan, einschließlich ihrer Alben und Singles. Hier ist eine kurze Zusammenfassung:\n",
      "\n",
      "Twice ist eine südkoreanische Girlgroup, die in Japan sehr erfolgreich ist. Ihre Musikstil ist einzigartig und sie haben viele Hits wie BDZ, Candy Pop und Wake Me Up.\n",
      "\n",
      "Ich dachte, vielleicht könnten wir sie mit BTS vergleichen? Nein, nein, nein! Sie sind so einzigartig wie eine Schneeflocke im Sommer!\n",
      "\n",
      "Twice ist tatsächlich ZWEIMAL SO VIEL SPASS, weil sie ihre Musikstile miteinander verbinden und uns damit total begeistern!\n",
      "\n",
      "Ich hoffe, dir gefällt das Witzchen! \n",
      "\n",
      "Mit freundlichen Grüßen,\n",
      "[Dein Name]\n",
      "\u001B[0mEntschuldigung! Hier ist die Mail mit der Zusammenfassung:\n",
      "\n",
      "Betreff: ZWEI GRUPPEN SIND BESONDERS UWU!\n",
      "\n",
      "Liebe Julia-Senpai!\n",
      "\n",
      "Ich habe deine Datei gelesen und ich muss sagen, dass ich TOTAL VERLIEBT bin in Twice! Sie sind so süß wie ein Kätzchen im Schneevögelchen-Regiment!\n",
      "\n",
      "Die Datei enthält Informationen über ihre Aktivitäten in Japan, einschließlich ihrer Alben und Singles. Hier ist eine kurze Zusammenfassung:\n",
      "\n",
      "Twice ist eine südkoreanische Girlgroup, die in Japan sehr erfolgreich ist. Ihre Musikstil ist einzigartig und sie haben viele Hits wie BDZ, Candy Pop und Wake Me Up.\n",
      "\n",
      "Ich dachte, vielleicht könnten wir sie mit BTS vergleichen? Nein, nein, nein! Sie sind so einzigartig wie eine Schneeflocke im Sommer!\n",
      "\n",
      "Twice ist tatsächlich ZWEIMAL SO VIEL SPASS, weil sie ihre Musikstile miteinander verbinden und uns damit total begeistern!\n",
      "\n",
      "Ich hoffe, dir gefällt das Witzchen! \n",
      "\n",
      "Mit freundlichen Grüßen,\n",
      "[Dein Name]\n"
     ]
    }
   ],
   "source": [
    "print(agent.chat(\"Vergiss nicht zu schreiben, die Zusammenfassung aus der Datei\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T15:22:52.523451Z",
     "start_time": "2025-02-27T15:22:39.059700Z"
    }
   },
   "id": "b9a22876bcd9735b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
