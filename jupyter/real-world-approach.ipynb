{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-06T19:25:34.180905Z",
     "start_time": "2025-03-06T19:25:34.082526Z"
    }
   },
   "source": [
    "from typing import Optional, List\n",
    "\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.storage.chat_store.postgres import PostgresChatStore\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "import pandas as pd\n",
    "from llama_index.experimental.query_engine import PandasQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata, FunctionTool\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core.indices import VectorStoreIndex\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import nest_asyncio\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.query_engine import BaseQueryEngine\n",
    "import uuid\n",
    "import chromadb\n",
    "\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from llama_index.core.vector_stores import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    "    FilterOperator,\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Init for APP\n",
    "\"\"\"\n",
    "nest_asyncio.apply()\n",
    "chats = {}\n",
    "chat_files = {}\n",
    "\n",
    "# LLM and Embedding settings\n",
    "llm = Ollama(model=\"llama3.1\")\n",
    "embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "# vector DB\n",
    "chroma_client = chromadb.HttpClient()\n",
    "chroma_collection = chroma_client.get_or_create_collection(os.getenv(\"CHROMA_COLLECTION_NAME\"))\n",
    "chroma_vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# postgres DB\n",
    "chat_store = PostgresChatStore.from_uri(\n",
    "    uri=\"postgresql+asyncpg://postgres:password@127.0.0.1:5432/llama-rag\",\n",
    ")\n",
    "\n",
    "class PandasTool(BaseModel):\n",
    "    pandas_query_engine: PandasQueryEngine\n",
    "    file_name: str\n",
    "    async def apandas_tool(self, query: str):\n",
    "        \"\"\"Executes a query with Pandas and return the string result\"\"\"\n",
    "        try:\n",
    "            result = await self.pandas_query_engine.aquery(query)\n",
    "            return str(result.response)  # Ensures only the output is returned\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "class ChatFile(BaseModel):\n",
    "    id: str\n",
    "    user_id: str\n",
    "    chat_id: str\n",
    "    file_name: str\n",
    "    path: str\n",
    "    mime_type: str\n",
    "\n",
    "class Chat(BaseModel):\n",
    "    chat_id: str\n",
    "    user_id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    context: str\n",
    "    chat_files: Optional[List[ChatFile]] = None\n",
    "\n",
    "class ToolsCollection(BaseModel):\n",
    "    pd_tools: List[PandasTool]\n",
    "    query_engines: List[BaseQueryEngine]\n",
    "    files: List[ChatFile]\n",
    "\n",
    "def index_uploaded_file(chat_file: ChatFile):\n",
    "    documents = SimpleDirectoryReader(input_files=[chat_file.path]).load_data()\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    VectorStoreIndex.from_documents(documents=documents,\n",
    "                                            storage_context=storage_context,\n",
    "                                            vector_store=vector_store, show_progress=True, embedding=Settings.embed_model)\n",
    "\n",
    "def upload_file_to_chat(file, chat_id: str):\n",
    "    # get chat\n",
    "    chat: Chat = chats.get(chat_id)\n",
    "    chat_file_id = uuid.uuid4()\n",
    "    chat_file = ChatFile(\n",
    "        id=chat_file_id,\n",
    "        # TODO: change ID to session or Payload\n",
    "        user_id=f\"\", # get the id by the session or payload\n",
    "        mime_type=file.mime_type,\n",
    "        file_name=file.file_name,\n",
    "        path=f\"/uploads/{chat_id}/{file.file_name}\",\n",
    "        chat_id=chat_id,\n",
    "    )\n",
    "    # set the file to the chat, updates the chat\n",
    "    # TODO: change to DB\n",
    "    chat.chat_files.append(file)\n",
    "    chats[chat_id] = chat\n",
    "    # save the ChatFile\n",
    "    # TODO: change to DB storage\n",
    "    chat_files[chat_file_id] = chat_file\n",
    "    # async, do something behind the request\n",
    "    # index the file\n",
    "    index_uploaded_file(chat_file)\n",
    "    return chat\n",
    "\n",
    "def create_chat(title: str, description: str, context: str):\n",
    "    \"\"\"\n",
    "    When the user first creates a chat. Create a chat entry.\n",
    "    :param title:\n",
    "    :param description:\n",
    "    :param context:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    chat_id = uuid.uuid4()\n",
    "    # TODO: get the user id later from the session or payload\n",
    "    chat = Chat(chat_id=str(chat_id), title=title, description=description, context=context, user_id=f\"user-id\")\n",
    "    chats[chat_id] = chat\n",
    "    return chat\n",
    "\n",
    "def create_filters_for_files(files: List[ChatFile]):\n",
    "    filters = [\n",
    "        MetadataFilters(\n",
    "            filters=[\n",
    "                MetadataFilter(\n",
    "                    operator=FilterOperator.EQ,\n",
    "                    key=\"file_id\",\n",
    "                    value=file.id,\n",
    "                )\n",
    "            ]\n",
    "        ) for file in files\n",
    "    ]\n",
    "    return filters\n",
    "\n",
    "def get_query_engines_from_filters(filters: List[MetadataFilters]):\n",
    "    storage_context = StorageContext.from_defaults(vector_store=chroma_vector_store)\n",
    "    vector_index = VectorStoreIndex.from_vector_store(vector_store=chroma_vector_store, storage_context=storage_context, embed_model=Settings.embed_model)\n",
    "    query_engines = [\n",
    "        vector_index.as_query_engine(filters=_filters) for _filters in filters\n",
    "    ]\n",
    "    return query_engines\n",
    "\n",
    "def get_pandas_tools_from_files(files: List[ChatFile]):\n",
    "    pd_tools = []\n",
    "    for file in files:\n",
    "        if file.mime_type == \"application/csv\":\n",
    "            pd_tool = PandasTool(pandas_query_engine=PandasQueryEngine(df=pd.read_csv(file.path, verbose=True)), file_name=file.file_name)\n",
    "            pd_tools.append(pd_tool)\n",
    "        if file.mime_type == \"application/excel\":\n",
    "            pd_tool = PandasTool(pandas_query_engine=PandasQueryEngine(df=pd.read_excel(file.path, verbose=True)), file_name=file.file_name)\n",
    "            pd_tools.append(pd_tool)\n",
    "    return pd_tools\n",
    "\n",
    "def aggregate_tools_from_collection(collection: ToolsCollection):\n",
    "    tools = [\n",
    "        QueryEngineTool(\n",
    "            query_engine=query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=f\"query_engine_{i}\",\n",
    "                description=f\"Queries through the document {collection.files[i].file_name}\",\n",
    "            )\n",
    "        ) for i, query_engine in enumerate(collection.query_engines)\n",
    "    ]\n",
    "    pd_tools = [\n",
    "        FunctionTool.from_defaults(\n",
    "            async_fn=pd_tool.apandas_tool,\n",
    "            name=f\"pandas_tool_{[i]}\",\n",
    "            description=f\"Pandas query tool for the spreadsheet {pd_tool.file_name}\",\n",
    "        ) for i, pd_tool in enumerate(collection.pd_tools)\n",
    "    ]\n",
    "    tools = tools + pd_tools\n",
    "    return tools\n",
    "\n",
    "async def chat_with_llm(query: str, chat_id: str):\n",
    "    \"\"\"\n",
    "    Sends a query to the chat for communicating with the LLM.\n",
    "    :param query:\n",
    "    :param chat_id:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # get the chat by its ID from the DB\n",
    "    chat = chats[chat_id]\n",
    "    chat_memory = ChatMemoryBuffer(\n",
    "        token_limit=5000,\n",
    "        chat_store=chat_store,\n",
    "        chat_store_key=chat_id,\n",
    "    )\n",
    "\n",
    "    # get the files by the chat_id and create filters\n",
    "    files: List[ChatFile] = chat.chat_files\n",
    "    filters = create_filters_for_files(files)\n",
    "\n",
    "    # builds index and get query engines from the filters\n",
    "    query_engines = get_query_engines_from_filters(filters=filters)\n",
    "\n",
    "    # check if file is csv/xcsl for Pandas Tool\n",
    "    pd_tools = get_pandas_tools_from_files(files=files)\n",
    "\n",
    "    tools_collection = ToolsCollection(pd_tools=pd_tools, query_engines=query_engines, files=files)\n",
    "    tools = aggregate_tools_from_collection(collection=tools_collection)\n",
    "\n",
    "    # create an agent to work with\n",
    "    agent = ReActAgent.from_tools(\n",
    "        tools=tools,\n",
    "        llm=Settings.llm,\n",
    "        memory=chat_memory,\n",
    "        verbose=True,\n",
    "        max_iterations=20,\n",
    "    )\n",
    "    system_prompt = PromptTemplate(chat.context)\n",
    "    agent.update_prompts({\"agent_worker:system_prompt\": system_prompt})\n",
    "    response = await agent.achat(query=query)\n",
    "    return response"
   ],
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 153 (2388024663.py, line 156)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[2], line 156\u001B[0;36m\u001B[0m\n\u001B[0;31m    async def chat_with_llm(query: str, chat_id: str):\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m expected an indented block after function definition on line 153\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
