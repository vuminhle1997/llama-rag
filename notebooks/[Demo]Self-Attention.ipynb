{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee555cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bde0dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a700c17",
   "metadata": {},
   "source": [
    "# Self-Attention Mechanism for One Head\n",
    "\n",
    "Self-Attention is a mechanism introduced in the paper \"Attention is All You Need\" to compute relationships between elements in a sequence. For a simple vocabulary like \"I am Max\" with dimensions of 768, the process involves:\n",
    "\n",
    "1. **Input Representation**: Each word is represented as a vector of size 768.\n",
    "2. **Query, Key, and Value Matrices**: These (Q, K, V) are randomly initialized for the weights based on the sequence length (L) and the dimensions (768).\n",
    "\n",
    "3. **Scaled Dot-Product Attention**:\n",
    "   - Compute the dot product between the Query and Key matrices.\n",
    "   - Scale the result by the square root of the dimension (768).\n",
    "   - Apply a mask to ensure causality (future words do not influence past words).\n",
    "   - Use the softmax function to normalize the scores.\n",
    "4. **Weighted Sum**: Multiply the attention scores with the Value matrix to get the final representation.\n",
    "\n",
    "This mechanism is important because it allows the model to focus on relevant parts of the input sequence, enabling better understanding and generation of context-aware outputs.\n",
    "\n",
    "---\n",
    "\n",
    "*This demonstration of how self-attention works was inspired by [this video](https://www.youtube.com/watch?v=QCJQG4DuHT0&t=3s), [this repository](https://github.com/ajhalthor/Transformer-Neural-Network), and the instructor [Ajay Halthor](https://github.com/ajhalthor).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1c687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "L, d_k, d_v = 3, 768, 768\n",
    "\n",
    "Q = np.random.rand(L, d_k)\n",
    "K = np.random.rand(L, d_k)\n",
    "V = np.random.rand(L, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d812292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.443965</td>\n",
       "      <td>0.552489</td>\n",
       "      <td>0.795881</td>\n",
       "      <td>0.912014</td>\n",
       "      <td>0.927523</td>\n",
       "      <td>0.928838</td>\n",
       "      <td>0.981990</td>\n",
       "      <td>0.415161</td>\n",
       "      <td>0.434954</td>\n",
       "      <td>0.841408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664132</td>\n",
       "      <td>0.549578</td>\n",
       "      <td>0.961703</td>\n",
       "      <td>0.434516</td>\n",
       "      <td>0.254016</td>\n",
       "      <td>0.601373</td>\n",
       "      <td>0.726527</td>\n",
       "      <td>0.299726</td>\n",
       "      <td>0.975034</td>\n",
       "      <td>0.543424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.147352</td>\n",
       "      <td>0.662447</td>\n",
       "      <td>0.347690</td>\n",
       "      <td>0.443408</td>\n",
       "      <td>0.526462</td>\n",
       "      <td>0.415522</td>\n",
       "      <td>0.920554</td>\n",
       "      <td>0.846130</td>\n",
       "      <td>0.011775</td>\n",
       "      <td>0.869826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067864</td>\n",
       "      <td>0.930125</td>\n",
       "      <td>0.102429</td>\n",
       "      <td>0.006717</td>\n",
       "      <td>0.744869</td>\n",
       "      <td>0.326065</td>\n",
       "      <td>0.079725</td>\n",
       "      <td>0.596883</td>\n",
       "      <td>0.053978</td>\n",
       "      <td>0.305716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.134103</td>\n",
       "      <td>0.689522</td>\n",
       "      <td>0.260076</td>\n",
       "      <td>0.682714</td>\n",
       "      <td>0.257299</td>\n",
       "      <td>0.498879</td>\n",
       "      <td>0.338012</td>\n",
       "      <td>0.475656</td>\n",
       "      <td>0.439719</td>\n",
       "      <td>0.145602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032548</td>\n",
       "      <td>0.853616</td>\n",
       "      <td>0.119093</td>\n",
       "      <td>0.344746</td>\n",
       "      <td>0.136598</td>\n",
       "      <td>0.763248</td>\n",
       "      <td>0.069965</td>\n",
       "      <td>0.829748</td>\n",
       "      <td>0.818024</td>\n",
       "      <td>0.475082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.443965  0.552489  0.795881  0.912014  0.927523  0.928838  0.981990   \n",
       "1  0.147352  0.662447  0.347690  0.443408  0.526462  0.415522  0.920554   \n",
       "2  0.134103  0.689522  0.260076  0.682714  0.257299  0.498879  0.338012   \n",
       "\n",
       "        7         8         9    ...       758       759       760       761  \\\n",
       "0  0.415161  0.434954  0.841408  ...  0.664132  0.549578  0.961703  0.434516   \n",
       "1  0.846130  0.011775  0.869826  ...  0.067864  0.930125  0.102429  0.006717   \n",
       "2  0.475656  0.439719  0.145602  ...  0.032548  0.853616  0.119093  0.344746   \n",
       "\n",
       "        762       763       764       765       766       767  \n",
       "0  0.254016  0.601373  0.726527  0.299726  0.975034  0.543424  \n",
       "1  0.744869  0.326065  0.079725  0.596883  0.053978  0.305716  \n",
       "2  0.136598  0.763248  0.069965  0.829748  0.818024  0.475082  \n",
       "\n",
       "[3 rows x 768 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ccb9fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.967386</td>\n",
       "      <td>0.641188</td>\n",
       "      <td>0.334244</td>\n",
       "      <td>0.363748</td>\n",
       "      <td>0.462085</td>\n",
       "      <td>0.423704</td>\n",
       "      <td>0.646040</td>\n",
       "      <td>0.610683</td>\n",
       "      <td>0.529257</td>\n",
       "      <td>0.408328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.825135</td>\n",
       "      <td>0.812598</td>\n",
       "      <td>0.196644</td>\n",
       "      <td>0.844694</td>\n",
       "      <td>0.498152</td>\n",
       "      <td>0.800483</td>\n",
       "      <td>0.981069</td>\n",
       "      <td>0.741634</td>\n",
       "      <td>0.460788</td>\n",
       "      <td>0.702955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.088451</td>\n",
       "      <td>0.325549</td>\n",
       "      <td>0.519656</td>\n",
       "      <td>0.389377</td>\n",
       "      <td>0.756308</td>\n",
       "      <td>0.851370</td>\n",
       "      <td>0.752869</td>\n",
       "      <td>0.095203</td>\n",
       "      <td>0.971188</td>\n",
       "      <td>0.700056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851865</td>\n",
       "      <td>0.328944</td>\n",
       "      <td>0.698538</td>\n",
       "      <td>0.422627</td>\n",
       "      <td>0.499308</td>\n",
       "      <td>0.908873</td>\n",
       "      <td>0.400249</td>\n",
       "      <td>0.785511</td>\n",
       "      <td>0.446014</td>\n",
       "      <td>0.601970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.246552</td>\n",
       "      <td>0.821833</td>\n",
       "      <td>0.760850</td>\n",
       "      <td>0.730059</td>\n",
       "      <td>0.710080</td>\n",
       "      <td>0.369434</td>\n",
       "      <td>0.826794</td>\n",
       "      <td>0.761943</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.233502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884720</td>\n",
       "      <td>0.888363</td>\n",
       "      <td>0.819361</td>\n",
       "      <td>0.864076</td>\n",
       "      <td>0.938706</td>\n",
       "      <td>0.635196</td>\n",
       "      <td>0.966073</td>\n",
       "      <td>0.603802</td>\n",
       "      <td>0.364685</td>\n",
       "      <td>0.609665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.967386  0.641188  0.334244  0.363748  0.462085  0.423704  0.646040   \n",
       "1  0.088451  0.325549  0.519656  0.389377  0.756308  0.851370  0.752869   \n",
       "2  0.246552  0.821833  0.760850  0.730059  0.710080  0.369434  0.826794   \n",
       "\n",
       "        7         8         9    ...       758       759       760       761  \\\n",
       "0  0.610683  0.529257  0.408328  ...  0.825135  0.812598  0.196644  0.844694   \n",
       "1  0.095203  0.971188  0.700056  ...  0.851865  0.328944  0.698538  0.422627   \n",
       "2  0.761943  0.237624  0.233502  ...  0.884720  0.888363  0.819361  0.864076   \n",
       "\n",
       "        762       763       764       765       766       767  \n",
       "0  0.498152  0.800483  0.981069  0.741634  0.460788  0.702955  \n",
       "1  0.499308  0.908873  0.400249  0.785511  0.446014  0.601970  \n",
       "2  0.938706  0.635196  0.966073  0.603802  0.364685  0.609665  \n",
       "\n",
       "[3 rows x 768 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56bf9a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.300657</td>\n",
       "      <td>0.174889</td>\n",
       "      <td>0.550840</td>\n",
       "      <td>0.542541</td>\n",
       "      <td>0.713070</td>\n",
       "      <td>0.654922</td>\n",
       "      <td>0.014812</td>\n",
       "      <td>0.846204</td>\n",
       "      <td>0.020754</td>\n",
       "      <td>0.318203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802917</td>\n",
       "      <td>0.550429</td>\n",
       "      <td>0.412732</td>\n",
       "      <td>0.671187</td>\n",
       "      <td>0.546453</td>\n",
       "      <td>0.680948</td>\n",
       "      <td>0.906085</td>\n",
       "      <td>0.210110</td>\n",
       "      <td>0.048897</td>\n",
       "      <td>0.968737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.641522</td>\n",
       "      <td>0.353871</td>\n",
       "      <td>0.485978</td>\n",
       "      <td>0.025493</td>\n",
       "      <td>0.029522</td>\n",
       "      <td>0.296605</td>\n",
       "      <td>0.304838</td>\n",
       "      <td>0.853458</td>\n",
       "      <td>0.016998</td>\n",
       "      <td>0.996952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704518</td>\n",
       "      <td>0.606242</td>\n",
       "      <td>0.363611</td>\n",
       "      <td>0.330029</td>\n",
       "      <td>0.762868</td>\n",
       "      <td>0.375294</td>\n",
       "      <td>0.550619</td>\n",
       "      <td>0.049723</td>\n",
       "      <td>0.170887</td>\n",
       "      <td>0.129178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.422075</td>\n",
       "      <td>0.718450</td>\n",
       "      <td>0.209494</td>\n",
       "      <td>0.852302</td>\n",
       "      <td>0.493842</td>\n",
       "      <td>0.602548</td>\n",
       "      <td>0.880430</td>\n",
       "      <td>0.345424</td>\n",
       "      <td>0.430350</td>\n",
       "      <td>0.797527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946100</td>\n",
       "      <td>0.317563</td>\n",
       "      <td>0.387658</td>\n",
       "      <td>0.094867</td>\n",
       "      <td>0.760170</td>\n",
       "      <td>0.209812</td>\n",
       "      <td>0.134985</td>\n",
       "      <td>0.082697</td>\n",
       "      <td>0.142534</td>\n",
       "      <td>0.241684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.300657  0.174889  0.550840  0.542541  0.713070  0.654922  0.014812   \n",
       "1  0.641522  0.353871  0.485978  0.025493  0.029522  0.296605  0.304838   \n",
       "2  0.422075  0.718450  0.209494  0.852302  0.493842  0.602548  0.880430   \n",
       "\n",
       "        7         8         9    ...       758       759       760       761  \\\n",
       "0  0.846204  0.020754  0.318203  ...  0.802917  0.550429  0.412732  0.671187   \n",
       "1  0.853458  0.016998  0.996952  ...  0.704518  0.606242  0.363611  0.330029   \n",
       "2  0.345424  0.430350  0.797527  ...  0.946100  0.317563  0.387658  0.094867   \n",
       "\n",
       "        762       763       764       765       766       767  \n",
       "0  0.546453  0.680948  0.906085  0.210110  0.048897  0.968737  \n",
       "1  0.762868  0.375294  0.550619  0.049723  0.170887  0.129178  \n",
       "2  0.760170  0.209812  0.134985  0.082697  0.142534  0.241684  \n",
       "\n",
       "[3 rows x 768 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2596de1",
   "metadata": {},
   "source": [
    "Scaling the dot product of Q and K by $\\sqrt{d_k}$ keeps the attention scores in a reasonable range, making the softmax more stable and improving training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af110d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "scaled = np.matmul(Q, K.T) / math.sqrt(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ddb4643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.714893</td>\n",
       "      <td>6.465850</td>\n",
       "      <td>6.558512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.770609</td>\n",
       "      <td>6.569006</td>\n",
       "      <td>6.534449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.892509</td>\n",
       "      <td>6.548279</td>\n",
       "      <td>6.485003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  6.714893  6.465850  6.558512\n",
       "1  6.770609  6.569006  6.534449\n",
       "2  6.892509  6.548279  6.485003"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a05815e",
   "metadata": {},
   "source": [
    "In the code above, we create a **mask matrix** `M` to enforce causality in the self-attention mechanism. The mask is a lower triangular matrix where the upper diagonal elements are set to $-\\infty$. This ensures that, when added to the attention scores before applying the softmax, the softmax output for those positions becomes zero. As a result, each position in the sequence can only attend to itself and previous positions, not to any future positions. This is crucial for tasks like language modeling, where future information should not be accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26d2f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.tril(np.ones((L,L)))\n",
    "M[M== 0] = -np.inf\n",
    "M[M == 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "171db727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2\n",
       "0  0.0 -inf -inf\n",
       "1  0.0  0.0 -inf\n",
       "2  0.0  0.0  0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d474df0",
   "metadata": {},
   "source": [
    "After computing the scaled dot product of $Q$ and $K$ (i.e., $(QK^T)/\\sqrt{d_k}$), we add the mask matrix $M$ to the result. This mask ensures that each position in the sequence can only attend to itself and previous positions, not future ones. The masked and scaled attention scores are then passed through the softmax function, which converts them into a probability distribution. This distribution determines how much focus (attention) each word should give to every other word in the sequence, while respecting the causality constraint imposed by the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f001373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "attention = softmax(scaled + M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc02df",
   "metadata": {},
   "source": [
    "After obtaining the softmax distribution (the attention weights), we multiply it by the Value matrix $V$ to produce the final output of the self-attention mechanism:\n",
    "\n",
    "$$\\text{Output} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right) \\cdot V$$\n",
    "\n",
    "**Why is this multiplication needed?**\n",
    "\n",
    "- The softmax distribution tells us how much attention each word in the sequence should pay to every other word (including itself).\n",
    "- By multiplying these attention weights with the Value matrix $V$, we compute a weighted sum of the value vectors for each position in the sequence.\n",
    "- This means each output vector is a blend of the value vectors, where the contribution of each value is determined by the attention score.\n",
    "- This allows the model to aggregate information from relevant positions in the sequence, enabling it to capture dependencies and context effectively.\n",
    "\n",
    "In summary, this multiplication enables the self-attention mechanism to produce context-aware representations for each word, based on how much attention it gives to other words in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42a4a84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052224</td>\n",
       "      <td>0.030378</td>\n",
       "      <td>0.095681</td>\n",
       "      <td>0.094239</td>\n",
       "      <td>0.123860</td>\n",
       "      <td>0.113760</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>0.146985</td>\n",
       "      <td>0.003605</td>\n",
       "      <td>0.055272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139466</td>\n",
       "      <td>0.095609</td>\n",
       "      <td>0.071691</td>\n",
       "      <td>0.116585</td>\n",
       "      <td>0.094919</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.157386</td>\n",
       "      <td>0.036496</td>\n",
       "      <td>0.008493</td>\n",
       "      <td>0.168269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.151522</td>\n",
       "      <td>0.085242</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.103466</td>\n",
       "      <td>0.135389</td>\n",
       "      <td>0.164804</td>\n",
       "      <td>0.048483</td>\n",
       "      <td>0.283529</td>\n",
       "      <td>0.006363</td>\n",
       "      <td>0.208102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253220</td>\n",
       "      <td>0.192097</td>\n",
       "      <td>0.130385</td>\n",
       "      <td>0.172809</td>\n",
       "      <td>0.214880</td>\n",
       "      <td>0.181397</td>\n",
       "      <td>0.249064</td>\n",
       "      <td>0.046052</td>\n",
       "      <td>0.034634</td>\n",
       "      <td>0.197303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.214962</td>\n",
       "      <td>0.187481</td>\n",
       "      <td>0.214652</td>\n",
       "      <td>0.233944</td>\n",
       "      <td>0.220438</td>\n",
       "      <td>0.262651</td>\n",
       "      <td>0.169419</td>\n",
       "      <td>0.348725</td>\n",
       "      <td>0.066204</td>\n",
       "      <td>0.322687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400753</td>\n",
       "      <td>0.247167</td>\n",
       "      <td>0.192598</td>\n",
       "      <td>0.200867</td>\n",
       "      <td>0.330464</td>\n",
       "      <td>0.225413</td>\n",
       "      <td>0.287572</td>\n",
       "      <td>0.062315</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>0.253328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.052224  0.030378  0.095681  0.094239  0.123860  0.113760  0.002573   \n",
       "1  0.151522  0.085242  0.174118  0.103466  0.135389  0.164804  0.048483   \n",
       "2  0.214962  0.187481  0.214652  0.233944  0.220438  0.262651  0.169419   \n",
       "\n",
       "        7         8         9    ...       758       759       760       761  \\\n",
       "0  0.146985  0.003605  0.055272  ...  0.139466  0.095609  0.071691  0.116585   \n",
       "1  0.283529  0.006363  0.208102  ...  0.253220  0.192097  0.130385  0.172809   \n",
       "2  0.348725  0.066204  0.322687  ...  0.400753  0.247167  0.192598  0.200867   \n",
       "\n",
       "        762       763       764       765       766       767  \n",
       "0  0.094919  0.118280  0.157386  0.036496  0.008493  0.168269  \n",
       "1  0.214880  0.181397  0.249064  0.046052  0.034634  0.197303  \n",
       "2  0.330464  0.225413  0.287572  0.062315  0.054945  0.253328  \n",
       "\n",
       "[3 rows x 768 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_V = np.matmul(attention, V)\n",
    "pd.DataFrame(new_V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
