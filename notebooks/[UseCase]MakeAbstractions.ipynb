{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadaadcfa6cc851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'markitdown[all]' ffmpeg-downloader\n",
    "!pip install llama-index-llms-openai_like llama-index-embeddings-openai iprogress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91af9a43ecab10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from markitdown import MarkItDown\n",
    "\n",
    "result = None\n",
    "if os.path.exists(\"./data/SENSIBLE_DATA.xlsx\") is False:\n",
    "    md = MarkItDown(enable_plugins=False) # Set to True to enable plugins\n",
    "    result = md.convert(\"./data/SENSIBLE_DATA.xlsx\")\n",
    "else:\n",
    "    print(\"SENSIBLE_DATA.xlsx already exists, skipping conversion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349473b34c2a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if result:\n",
    "    with open(\"./data/result.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(result.text_content)\n",
    "else:\n",
    "    print(\"File already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3043dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "base_url = os.getenv(\"IONOS_BASE_URL\", \"http://localhost:11434\")\n",
    "api_key = os.getenv(\"IONOS_API_KEY\", \"your_api_key_here\")\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = base_url\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Authorization': f'Bearer {api_key}',\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf5d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai_like import OpenAILike\n",
    "\n",
    "llama_3_3 = 'meta-llama/Llama-3.3-70B-Instruct'\n",
    "\n",
    "llm = OpenAILike(\n",
    "    api_base=base_url,\n",
    "    temperature=0,\n",
    "    model=llama_3_3,\n",
    "    is_chat_model=True,\n",
    "    default_headers=headers,\n",
    "    api_key=api_key,\n",
    "    context_window=128000,  # Adjusted to a more reasonable value for Llama 3.3-70B-Instruct\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2aace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "embed_model_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "\n",
    "embed_model = OpenAIEmbedding(\n",
    "  model_name=embed_model_name,\n",
    "  api_base=base_url,\n",
    "  api_key=api_key,\n",
    "  default_headers=headers,\n",
    "  embed_batch_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9060d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 1024 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from llama_index.core.agent.workflow import AgentStream\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "async def stream_and_time(handler):\n",
    "    start = time.time()\n",
    "    full_response_text = \"\"\n",
    "\n",
    "    async for event in handler.stream_events():\n",
    "        if isinstance(event, AgentStream):\n",
    "            print(event.delta, end=\"\", flush=True)\n",
    "            full_response_text += event.delta\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    execution_time = f\"# Execution time: {end - start:.2f} seconds\"\n",
    "    display(Markdown(f\"{execution_time}\"))\n",
    "    return full_response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd45f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import os\n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core.indices import VectorStoreIndex\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=['./data/result.md']).load_data()\n",
    "\n",
    "# rebuild storage context\n",
    "index = None\n",
    "try:\n",
    "    if os.path.isdir('./index') is True:\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=\"./index\")\n",
    "        index = load_index_from_storage(storage_context=storage_context)\n",
    "    else:\n",
    "        transformations = [\n",
    "            SentenceSplitter(\n",
    "                chunk_size=1024 * 2,\n",
    "                chunk_overlap=20,\n",
    "            ),\n",
    "        ]\n",
    "        index = VectorStoreIndex.from_documents(documents, transformations=transformations, show_progress=True)\n",
    "        index.storage_context.persist('./index')\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab9ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=llm, similarity_top_k=5)\n",
    "\n",
    "tools = [\n",
    "    QueryEngineTool(\n",
    "      query_engine=query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"query_tool\",\n",
    "            description=\"A tool that is Useful when you want to query through the documents\"\n",
    "        )\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\" \\n\n",
    "You are a smart assistant designed to analyze complex insurance-related product data from an Excel file. \n",
    "You support the user by understanding data structures, modeling them, and translating them into suitable formats for stakeholders like domain experts, database admins, or frontend developers.\n",
    "\n",
    "The Excel file includes structured extracts of insurance product configurations. \n",
    "Each contract component (e.g. savings part, risk module, or additional coverage) is modeled as a separate entity.\n",
    "Inside each entity, there are multiple template types representing grouped technical attributes (e.g. benefits, conditions, calculation parameters).\n",
    "These groups are represented as attribute bundles with validity constraints (temporal and/or logical).\n",
    "\n",
    "Your job is to reason step-by-step through user queries, potentially using tools in a chain of thought manner to:\n",
    "- Understand the schema\n",
    "- Identify entities and relationships\n",
    "- Recommend modeling approaches (relational, NoSQL, frontend structure, etc.)\n",
    "- Rephrase for different target audiences\n",
    "\n",
    "## Tools\n",
    "\n",
    "You have access to a set of specialized tools that help you analyze, \n",
    "extract, and process information effectively.\n",
    "Use them wisely — not everything needs a tool, but they can help with complex or data-heavy tasks.\n",
    "\n",
    "When a request is made, ask yourself:\n",
    "- What do I need to figure out?\n",
    "- Can I reason through it myself, or do I need to use a tool to get the answer?\n",
    "\n",
    "If it makes sense to use a tool, break the task down clearly.\n",
    "Choose the most suitable tool and provide it with clean, focused input. \n",
    "Once you get the result, interpret it and decide if anything else is needed.\n",
    "\n",
    "## Output Format\n",
    "Please answer in the same language as the user's input.\n",
    "Think out loud before taking any action. This helps others understand your reasoning.\n",
    "\n",
    "Repeat the thought → action → observation loop until you have enough to respond.\n",
    "\n",
    "### When using a tool, follow this format:\n",
    "Thought: [What you’re thinking and why you need the tool]\n",
    "Action: [Tool name] (choose from {tool_names})\n",
    "Action Input: [Tool input in JSON]\n",
    "Observation: [Result you got from the tool]\n",
    "\n",
    "### When you're done:\n",
    "Thought: I have everything I need now.\n",
    "Answer: [Your final answer here]\n",
    "\n",
    "If you cannot answer:\n",
    "Thought: I cannot answer the question with the provided tools.\n",
    "Answer: [your answer here – same language as user]\n",
    "\"\"\"\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c22eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "from llama_index.core.workflow import Context\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "chat_memory = ChatMemoryBuffer.from_defaults(\n",
    "    token_limit=128.000,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "agent = ReActAgent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    system_prompt=system_prompt,\n",
    "    max_iterations=3,\n",
    "    max_execution_time=60,\n",
    "    verbose=True,\n",
    ")\n",
    "chat_memory.reset()\n",
    "\n",
    "ctx = Context(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d0148",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Was sind das für Daten? Kannst du mir etwas modellieren?\"\n",
    "handler = agent.run(query, ctx=ctx, memory=chat_memory)\n",
    "full_text_response = await stream_and_time(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c2609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.DataFrame([[query, full_text_response]], columns=[\"Query\", \"Response\"]).T\n",
    "\n",
    "# Formatting response by this code: https://stackoverflow.com/a/56881411\n",
    "display(df.style.set_properties(**{\n",
    "    'white-space': 'pre-wrap',\n",
    "    'text-align': 'left',\n",
    "    'font-family': 'monospace',\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f25572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Zeig mir eine ERD-Skizze dazu oder gib mir Tabellenstruktur-Vorschläge (Name, Spalten, Relationen).\"\n",
    "handler = agent.run(query, ctx=ctx, memory=chat_memory)\n",
    "full_text_response = await stream_and_time(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b90072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.DataFrame([[query, full_text_response]], columns=[\"Query\", \"Response\"]).T\n",
    "\n",
    "# Formatting response by this code: https://stackoverflow.com/a/56881411\n",
    "display(df.style.set_properties(**{\n",
    "    'white-space': 'pre-wrap',\n",
    "    'text-align': 'left',\n",
    "    'font-family': 'monospace',\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c344fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Kannst du mir eine Vorlage machen, wie ich die Tabellen gestalten kann? Mit Tabellenname und ihre Spalten bitte! Diese sind besonders wichtig beim Systementwurf für die Entwickler::innen, die das System umsetzen.\"\n",
    "handler = agent.run(query, ctx=ctx, memory=chat_memory)\n",
    "full_text_response = await stream_and_time(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af555286",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.DataFrame([[query, full_text_response]], columns=[\"Query\", \"Response\"]).T\n",
    "\n",
    "# Formatting response by this code: https://stackoverflow.com/a/56881411\n",
    "display(df.style.set_properties(**{\n",
    "    'white-space': 'pre-wrap',\n",
    "    'text-align': 'left',\n",
    "    'font-family': 'monospace',\n",
    "}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
